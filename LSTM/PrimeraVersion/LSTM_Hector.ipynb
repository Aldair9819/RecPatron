{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-28 14:21:35.590825: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-28 14:21:35.590852: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-28 14:21:35.591796: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-28 14:21:35.597190: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-28 14:21:36.318614: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos de ejemplo (secuencia de números)\n",
    "df = pd.read_excel('sector8TS.xls')  # Reemplaza 'datos.xlsx' con el nombre de tu archivo Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.217553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.211681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.181088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.181397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.165637</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      datos\n",
       "0  0.217553\n",
       "1  0.211681\n",
       "2  0.181088\n",
       "3  0.181397\n",
       "4  0.165637"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df['datos'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.21755263 0.21168112 0.1810878  0.18139689 0.16563669 0.10352298\n",
      " 0.0936341  0.08868985 0.0936341  0.20673669 0.48485779 0.67058094\n",
      " 0.69128558 0.77317667 0.5877626  0.43881336 0.41841781 0.45982689\n",
      " 0.43912244 0.38195298 0.43325094 0.43356002 0.35661318 0.19623001\n",
      " 0.13473428 0.1093943  0.05191594 0.04202725 0.07323856 0.09826946\n",
      " 0.21199021 0.34579724 0.55716928 0.69592093 0.85568593 0.83034613\n",
      " 0.69622983 0.62917186 0.61835592 0.65451166 0.67552538 0.44839315\n",
      " 0.44406669 0.44375779 0.5        0.52132262 0.30469724 0.20673669\n",
      " 0.14462297 0.05747836 0.02101372 0.01112484 0.04758967 0.05778745\n",
      " 0.20673669 0.36093945 0.58281835 0.62391834 0.67521629 0.62391834\n",
      " 0.56180463 0.47435112 0.5466626  0.54758967 0.4749691  0.45395557\n",
      " 0.44901113 0.37669964 0.50556242 0.55222502 0.29913482 0.2527813\n",
      " 0.17583447 0.06211371 0.01606928 0.03646483 0.05840543 0.04697169\n",
      " 0.14462297 0.30407907 0.54140908 0.63967854 0.65451166 0.60815814\n",
      " 0.50494444 0.57787391 0.5469715  0.66563651 0.50494444 0.46971576\n",
      " 0.49505556 0.41285539 0.40760205 0.55191594 0.36619279 0.20086536\n",
      " 0.19653891 0.0936341  0.05778745 0.03182948 0.03677391 0.07323856\n",
      " 0.15574782 0.38164408 0.47991354 0.66007408 0.78399261 0.69622983\n",
      " 0.59301612 0.55191594 0.55191594 0.62917186 0.62453651 0.67614337\n",
      " 0.54233615 0.29480837 0.34085299 0.34085299 0.24814595 0.18634114\n",
      " 0.21724355 0.16501852 0.07262057 0.08838077 0.07323856 0.0373919\n",
      " 0.07354764 0.08374542 0.2527813  0.62330036 0.81427685 0.88102591\n",
      " 0.80933241 0.63411611 0.38195298 0.35074167 0.38226207 0.44406669\n",
      " 0.36186652 0.34579724 0.36619279 0.38689742 0.27379484 0.21168112\n",
      " 0.19159466 0.08807168 0.04728059 0.06798522 0.05253411 0.05253411\n",
      " 0.04202725 0.10383188 0.2685415  0.50587151 0.80469706 0.82972796\n",
      " 0.65482075 0.5877626  0.53121131 0.57787391 0.52595797 0.4187269\n",
      " 0.42367115 0.39276893 0.41810872 0.38164408 0.30500614 0.22280597\n",
      " 0.19097649 0.11341167 0.07200259 0.02132262 0.05778745 0.02286786\n",
      " 0.02719413 0.05191594 0.2218789  0.51050686 0.83529038 0.86619279\n",
      " 0.75834354 0.73733001 0.77873909 0.63473428 0.50587151 0.47435112\n",
      " 0.58312725 0.5216317  0.56736705 0.46446224 0.26359706 0.25803464\n",
      " 0.17614337 0.05191594 0.02101372 0.04728059 0.07818299 0.07849208\n",
      " 0.26266999 0.5466626  0.61928299 0.52070463 0.60321389 0.60846723\n",
      " 0.49536465 0.53646483 0.57787391 0.55222502 0.50092707 0.50679858\n",
      " 0.44375779 0.47466002 0.58807168 0.58838077 0.34672431 0.21724355\n",
      " 0.15574782 0.02101372 0.01143393 0.06304078 0.04697169 0.04728059\n",
      " 0.29419038 0.5157602  0.65512983 0.55686038 0.72682315 0.6344252\n",
      " 0.59796037 0.47466002 0.46415334 0.52132262 0.40822004 0.36093945\n",
      " 0.49536465 0.50030909 0.618665   0.5624228  0.35043259 0.21199021\n",
      " 0.14956741 0.03152039 0.         0.04202725 0.05253411 0.07323856\n",
      " 0.26328798 0.59301612 0.67490721 0.53677373 0.5466626  0.5466626\n",
      " 0.49505556 0.49505556 0.49505556 0.52101353 0.48022244 0.46384426\n",
      " 0.56767614 0.58312725 0.65512983 0.58281835 0.34579724 0.2061187\n",
      " 0.17119911 0.04728059 0.00587151 0.00648949 0.03677391 0.05222502\n",
      " 0.25772556 0.52626705 0.69097649 0.58807168 0.66038317 0.70148335\n",
      " 0.71168112 0.63936964 0.56180463 0.53677373 0.49598263 0.5627317\n",
      " 0.52132262 0.60352279 0.70148335 0.5157602  0.41810872 0.21199021\n",
      " 0.18046982 0.07849208 0.04789876 0.06304078 0.05284301 0.07849208\n",
      " 0.29882574 0.57725592 0.65482075 0.53553776 0.57694684 0.62360944\n",
      " 0.49536465 0.49011131 0.5157602  0.54635352 0.53182948 0.4907293\n",
      " 0.47002466 0.49042021 0.50061799 0.39740428 0.33559946 0.18695931\n",
      " 0.14524114 0.0933252  0.03677391 0.06798522 0.03244746 0.04264524\n",
      " 0.08312743 0.16038317 0.42243518 0.64987631 0.76792333 0.69622983\n",
      " 0.60352279 0.60846723 0.5877626  0.60352279 0.52626705 0.49567373\n",
      " 0.49042021 0.40822004 0.46971576 0.41285539 0.30500614 0.23269466\n",
      " 0.23763909 0.1341163  0.12948095 0.0998147  0.1248456  0.09857854\n",
      " 0.11464782 0.15049448 0.28893705 0.51112484 0.85537703 0.94901113\n",
      " 0.82014835 0.71168112 0.63380721 0.63936964 0.51081576 0.49598263\n",
      " 0.57818299 0.5466626  0.64987631 0.58281835 0.35105076 0.28955503\n",
      " 0.21693446 0.08312743 0.04233634 0.05747836 0.08374542 0.06334987\n",
      " 0.26359706 0.62391834 0.65512983 0.5466626  0.61310258 0.55716928\n",
      " 0.5627317  0.59857854 0.59826946 0.5877626  0.42336225 0.40760205\n",
      " 0.4283065  0.49505556 0.59301612 0.5466626  0.31520392 0.23269466\n",
      " 0.16038317 0.07849208 0.06273188 0.06334987 0.06829431 0.07849208\n",
      " 0.32601986 0.60846723 0.70673669 0.51112484 0.61835592 0.5874537\n",
      " 0.47960445 0.46971576 0.52595797 0.61835592 0.53615574 0.52626705\n",
      " 0.55686038 0.54604443 0.65482075 0.59301612 0.38226207 0.21168112\n",
      " 0.20210133 0.06798522 0.03182948 0.04697169 0.09394319 0.06829431\n",
      " 0.22744132 0.65482075 0.68572315 0.49536465 0.4592089  0.51606928\n",
      " 0.52070463 0.50556242 0.52070463 0.54635352 0.57756482 0.52132262\n",
      " 0.618665   0.59857854 0.60352279 0.53152039 0.32045744 0.21199021\n",
      " 0.21693446 0.10877632 0.0624228  0.04728059 0.08838077 0.13998762\n",
      " 0.32014835 0.62917186 0.73733001 0.54140908 0.57849189 0.5466626\n",
      " 0.53677373 0.55129795 0.49505556 0.44870205 0.49042021 0.52626705\n",
      " 0.50556242 0.62886278 0.62391834 0.5877626  0.30500614]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos para LSTM\n",
    "def prepare_data(data, n_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data)):\n",
    "        end_ix = i + n_steps\n",
    "        if end_ix > len(data)-1:\n",
    "            break\n",
    "        seq_x, seq_y = data[i:end_ix], data[end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return tf.convert_to_tensor(X, dtype=tf.float32), tf.convert_to_tensor(y, dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-28 14:23:30.265391: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-28 14:23:30.299614: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-28 14:23:30.299873: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-28 14:23:30.301393: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-28 14:23:30.301741: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-28 14:23:30.301915: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-28 14:23:30.383301: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-28 14:23:30.383570: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-28 14:23:30.383754: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-28 14:23:30.383894: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4800 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# Definir hiperparámetros\n",
    "n_steps = 4\n",
    "n_features = 1\n",
    "\n",
    "# Preparar datos\n",
    "X, y = prepare_data(data, n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.21755263 0.21168113 0.1810878  0.18139689]\n",
      " [0.21168113 0.1810878  0.18139689 0.16563669]\n",
      " [0.1810878  0.18139689 0.16563669 0.10352298]\n",
      " ...\n",
      " [0.49042022 0.52626705 0.5055624  0.6288628 ]\n",
      " [0.52626705 0.5055624  0.6288628  0.62391835]\n",
      " [0.5055624  0.6288628  0.62391835 0.5877626 ]], shape=(451, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[0.16563669 0.10352298 0.09363411 0.08868986 0.09363411 0.20673668\n",
      " 0.4848578  0.6705809  0.69128555 0.77317667 0.5877626  0.43881336\n",
      " 0.4184178  0.4598269  0.43912244 0.38195297 0.43325093 0.43356\n",
      " 0.3566132  0.19623001 0.13473429 0.1093943  0.05191594 0.04202725\n",
      " 0.07323856 0.09826946 0.2119902  0.34579724 0.55716926 0.69592094\n",
      " 0.85568595 0.8303461  0.6962298  0.62917185 0.61835593 0.6545117\n",
      " 0.67552537 0.44839314 0.4440667  0.4437578  0.5        0.5213226\n",
      " 0.30469725 0.20673668 0.14462297 0.05747836 0.02101372 0.01112484\n",
      " 0.04758967 0.05778745 0.20673668 0.36093944 0.5828183  0.62391835\n",
      " 0.6752163  0.62391835 0.56180465 0.4743511  0.5466626  0.54758966\n",
      " 0.4749691  0.45395556 0.44901112 0.37669966 0.5055624  0.55222505\n",
      " 0.29913482 0.2527813  0.17583446 0.06211371 0.01606928 0.03646483\n",
      " 0.05840543 0.04697169 0.14462297 0.3040791  0.5414091  0.63967854\n",
      " 0.6545117  0.6081582  0.50494444 0.5778739  0.5469715  0.6656365\n",
      " 0.50494444 0.46971577 0.49505556 0.4128554  0.40760204 0.55191594\n",
      " 0.3661928  0.20086536 0.19653891 0.09363411 0.05778745 0.03182948\n",
      " 0.03677391 0.07323856 0.15574782 0.38164407 0.47991353 0.66007406\n",
      " 0.7839926  0.6962298  0.59301615 0.55191594 0.55191594 0.62917185\n",
      " 0.6245365  0.67614335 0.54233617 0.29480836 0.34085298 0.34085298\n",
      " 0.24814595 0.18634114 0.21724355 0.16501851 0.07262057 0.08838077\n",
      " 0.07323856 0.0373919  0.07354764 0.08374541 0.2527813  0.6233004\n",
      " 0.8142769  0.8810259  0.80933243 0.6341161  0.38195297 0.35074168\n",
      " 0.38226208 0.4440667  0.36186653 0.34579724 0.3661928  0.38689741\n",
      " 0.27379483 0.21168113 0.19159466 0.08807168 0.04728059 0.06798522\n",
      " 0.05253411 0.05253411 0.04202725 0.10383188 0.26854149 0.50587153\n",
      " 0.80469704 0.82972795 0.65482074 0.5877626  0.5312113  0.5778739\n",
      " 0.52595794 0.4187269  0.42367116 0.39276892 0.41810873 0.38164407\n",
      " 0.30500615 0.22280596 0.19097649 0.11341166 0.07200259 0.02132262\n",
      " 0.05778745 0.02286786 0.02719413 0.05191594 0.2218789  0.51050687\n",
      " 0.8352904  0.86619276 0.7583435  0.73733    0.7787391  0.6347343\n",
      " 0.50587153 0.4743511  0.58312726 0.5216317  0.5673671  0.46446225\n",
      " 0.26359707 0.25803465 0.17614336 0.05191594 0.02101372 0.04728059\n",
      " 0.078183   0.07849208 0.26266998 0.5466626  0.61928296 0.5207046\n",
      " 0.6032139  0.6084672  0.49536464 0.5364648  0.5778739  0.55222505\n",
      " 0.5009271  0.50679857 0.4437578  0.47466    0.5880717  0.58838075\n",
      " 0.3467243  0.21724355 0.15574782 0.02101372 0.01143393 0.06304079\n",
      " 0.04697169 0.04728059 0.29419038 0.5157602  0.65512985 0.5568604\n",
      " 0.72682315 0.6344252  0.59796035 0.47466    0.46415335 0.5213226\n",
      " 0.40822002 0.36093944 0.49536464 0.5003091  0.618665   0.5624228\n",
      " 0.35043257 0.2119902  0.14956741 0.03152039 0.         0.04202725\n",
      " 0.05253411 0.07323856 0.263288   0.59301615 0.6749072  0.53677374\n",
      " 0.5466626  0.5466626  0.49505556 0.49505556 0.49505556 0.52101356\n",
      " 0.48022243 0.46384427 0.5676761  0.58312726 0.65512985 0.5828183\n",
      " 0.34579724 0.2061187  0.17119911 0.04728059 0.00587151 0.00648949\n",
      " 0.03677391 0.05222502 0.25772557 0.52626705 0.6909765  0.5880717\n",
      " 0.66038316 0.70148337 0.7116811  0.6393696  0.56180465 0.53677374\n",
      " 0.49598265 0.5627317  0.5213226  0.6035228  0.70148337 0.5157602\n",
      " 0.41810873 0.2119902  0.18046981 0.07849208 0.04789876 0.06304079\n",
      " 0.05284301 0.07849208 0.29882574 0.5772559  0.65482074 0.5355378\n",
      " 0.57694685 0.6236094  0.49536464 0.49011132 0.5157602  0.5463535\n",
      " 0.5318295  0.4907293  0.47002468 0.49042022 0.500618   0.39740428\n",
      " 0.33559945 0.18695931 0.14524114 0.09332521 0.03677391 0.06798522\n",
      " 0.03244746 0.04264523 0.08312743 0.16038316 0.42243516 0.6498763\n",
      " 0.76792336 0.6962298  0.6035228  0.6084672  0.5877626  0.6035228\n",
      " 0.52626705 0.49567375 0.49042022 0.40822002 0.46971577 0.4128554\n",
      " 0.30500615 0.23269466 0.2376391  0.1341163  0.12948094 0.0998147\n",
      " 0.12484559 0.09857854 0.11464782 0.15049449 0.28893703 0.51112485\n",
      " 0.855377   0.94901115 0.82014835 0.7116811  0.63380724 0.6393696\n",
      " 0.51081574 0.49598265 0.578183   0.5466626  0.6498763  0.5828183\n",
      " 0.35105076 0.28955504 0.21693446 0.08312743 0.04233634 0.05747836\n",
      " 0.08374541 0.06334987 0.26359707 0.62391835 0.65512985 0.5466626\n",
      " 0.61310256 0.55716926 0.5627317  0.5985785  0.59826946 0.5877626\n",
      " 0.42336226 0.40760204 0.4283065  0.49505556 0.59301615 0.5466626\n",
      " 0.3152039  0.23269466 0.16038316 0.07849208 0.06273188 0.06334987\n",
      " 0.06829431 0.07849208 0.32601985 0.6084672  0.7067367  0.51112485\n",
      " 0.61835593 0.5874537  0.47960445 0.46971577 0.52595794 0.61835593\n",
      " 0.53615576 0.52626705 0.5568604  0.5460444  0.65482074 0.59301615\n",
      " 0.38226208 0.21168113 0.20210133 0.06798522 0.03182948 0.04697169\n",
      " 0.09394319 0.06829431 0.22744133 0.65482074 0.6857231  0.49536464\n",
      " 0.4592089  0.5160693  0.5207046  0.5055624  0.5207046  0.5463535\n",
      " 0.57756484 0.5213226  0.618665   0.5985785  0.6035228  0.53152037\n",
      " 0.32045743 0.2119902  0.21693446 0.10877632 0.0624228  0.04728059\n",
      " 0.08838077 0.13998762 0.32014835 0.62917185 0.73733    0.5414091\n",
      " 0.57849187 0.5466626  0.53677374 0.55129796 0.49505556 0.44870204\n",
      " 0.49042022 0.52626705 0.5055624  0.6288628  0.62391835 0.5877626\n",
      " 0.30500615], shape=(451,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "# Definir el modelo\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-28 14:23:43.358076: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f3a11a22a00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-04-28 14:23:43.358100: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti with Max-Q Design, Compute Capability 7.5\n",
      "2024-04-28 14:23:43.362770: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-04-28 14:23:43.376638: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1714339423.432728   51797 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 2s 10ms/step - loss: 0.1535\n",
      "Epoch 2/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0880\n",
      "Epoch 3/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0429\n",
      "Epoch 4/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0333\n",
      "Epoch 5/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0319\n",
      "Epoch 6/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0305\n",
      "Epoch 7/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0294\n",
      "Epoch 8/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0279\n",
      "Epoch 9/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0268\n",
      "Epoch 10/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0254\n",
      "Epoch 11/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0240\n",
      "Epoch 12/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0223\n",
      "Epoch 13/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0206\n",
      "Epoch 14/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0189\n",
      "Epoch 15/200\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0169\n",
      "Epoch 16/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0154\n",
      "Epoch 17/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0142\n",
      "Epoch 18/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0132\n",
      "Epoch 19/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0130\n",
      "Epoch 20/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0125\n",
      "Epoch 21/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0123\n",
      "Epoch 22/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0118\n",
      "Epoch 23/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0121\n",
      "Epoch 24/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0132\n",
      "Epoch 25/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0119\n",
      "Epoch 26/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0112\n",
      "Epoch 27/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0112\n",
      "Epoch 28/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0107\n",
      "Epoch 29/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0107\n",
      "Epoch 30/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0105\n",
      "Epoch 31/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0112\n",
      "Epoch 32/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0105\n",
      "Epoch 33/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0100\n",
      "Epoch 34/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0098\n",
      "Epoch 35/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0099\n",
      "Epoch 36/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0097\n",
      "Epoch 37/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0095\n",
      "Epoch 38/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0094\n",
      "Epoch 39/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0094\n",
      "Epoch 40/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0093\n",
      "Epoch 41/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0091\n",
      "Epoch 42/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0090\n",
      "Epoch 43/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0089\n",
      "Epoch 44/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0088\n",
      "Epoch 45/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0088\n",
      "Epoch 46/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0091\n",
      "Epoch 47/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0088\n",
      "Epoch 48/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0085\n",
      "Epoch 49/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0084\n",
      "Epoch 50/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0084\n",
      "Epoch 51/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0082\n",
      "Epoch 52/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0083\n",
      "Epoch 53/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0081\n",
      "Epoch 54/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0080\n",
      "Epoch 55/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0079\n",
      "Epoch 56/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0079\n",
      "Epoch 57/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0079\n",
      "Epoch 58/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0078\n",
      "Epoch 59/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0080\n",
      "Epoch 60/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0082\n",
      "Epoch 61/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0080\n",
      "Epoch 62/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0076\n",
      "Epoch 63/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0075\n",
      "Epoch 64/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0077\n",
      "Epoch 65/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0078\n",
      "Epoch 66/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0073\n",
      "Epoch 67/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0074\n",
      "Epoch 68/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0075\n",
      "Epoch 69/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0072\n",
      "Epoch 70/200\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0073\n",
      "Epoch 71/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0072\n",
      "Epoch 72/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0072\n",
      "Epoch 73/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0074\n",
      "Epoch 74/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0072\n",
      "Epoch 75/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0073\n",
      "Epoch 76/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0069\n",
      "Epoch 77/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0074\n",
      "Epoch 78/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0070\n",
      "Epoch 79/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0071\n",
      "Epoch 80/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0071\n",
      "Epoch 81/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0067\n",
      "Epoch 82/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0068\n",
      "Epoch 83/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0067\n",
      "Epoch 84/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0069\n",
      "Epoch 85/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0067\n",
      "Epoch 86/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0071\n",
      "Epoch 87/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0069\n",
      "Epoch 88/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0066\n",
      "Epoch 89/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0066\n",
      "Epoch 90/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0067\n",
      "Epoch 91/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0066\n",
      "Epoch 92/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0068\n",
      "Epoch 93/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0068\n",
      "Epoch 94/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0065\n",
      "Epoch 95/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0065\n",
      "Epoch 96/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0066\n",
      "Epoch 97/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0065\n",
      "Epoch 98/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0064\n",
      "Epoch 99/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0065\n",
      "Epoch 100/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0063\n",
      "Epoch 101/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0063\n",
      "Epoch 102/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0064\n",
      "Epoch 103/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0065\n",
      "Epoch 104/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0067\n",
      "Epoch 105/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0066\n",
      "Epoch 106/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0067\n",
      "Epoch 107/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0063\n",
      "Epoch 108/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0062\n",
      "Epoch 109/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0062\n",
      "Epoch 110/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0063\n",
      "Epoch 111/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0062\n",
      "Epoch 112/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0066\n",
      "Epoch 113/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0066\n",
      "Epoch 114/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0064\n",
      "Epoch 115/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0072\n",
      "Epoch 116/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0063\n",
      "Epoch 117/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0061\n",
      "Epoch 118/200\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0061\n",
      "Epoch 119/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0064\n",
      "Epoch 120/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0064\n",
      "Epoch 121/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0062\n",
      "Epoch 122/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0060\n",
      "Epoch 123/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0061\n",
      "Epoch 124/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0060\n",
      "Epoch 125/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0061\n",
      "Epoch 126/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0061\n",
      "Epoch 127/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0065\n",
      "Epoch 128/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0063\n",
      "Epoch 129/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0061\n",
      "Epoch 130/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0060\n",
      "Epoch 131/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0061\n",
      "Epoch 132/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0059\n",
      "Epoch 133/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0059\n",
      "Epoch 134/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0060\n",
      "Epoch 135/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0059\n",
      "Epoch 136/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0061\n",
      "Epoch 137/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0059\n",
      "Epoch 138/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0062\n",
      "Epoch 139/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0060\n",
      "Epoch 140/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0059\n",
      "Epoch 141/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0059\n",
      "Epoch 142/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0064\n",
      "Epoch 143/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0060\n",
      "Epoch 144/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0059\n",
      "Epoch 145/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0059\n",
      "Epoch 146/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0060\n",
      "Epoch 147/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0058\n",
      "Epoch 148/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0060\n",
      "Epoch 149/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0058\n",
      "Epoch 150/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0058\n",
      "Epoch 151/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0058\n",
      "Epoch 152/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0058\n",
      "Epoch 153/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0062\n",
      "Epoch 154/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0060\n",
      "Epoch 155/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0059\n",
      "Epoch 156/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0062\n",
      "Epoch 157/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0060\n",
      "Epoch 158/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0058\n",
      "Epoch 159/200\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0060\n",
      "Epoch 160/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0059\n",
      "Epoch 161/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0057\n",
      "Epoch 162/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0057\n",
      "Epoch 163/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0059\n",
      "Epoch 164/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0061\n",
      "Epoch 165/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0058\n",
      "Epoch 166/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0059\n",
      "Epoch 167/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0059\n",
      "Epoch 168/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0059\n",
      "Epoch 169/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0059\n",
      "Epoch 170/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0059\n",
      "Epoch 171/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0060\n",
      "Epoch 172/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0058\n",
      "Epoch 173/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0061\n",
      "Epoch 174/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0058\n",
      "Epoch 175/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0060\n",
      "Epoch 176/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0057\n",
      "Epoch 177/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0060\n",
      "Epoch 178/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0060\n",
      "Epoch 179/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0057\n",
      "Epoch 180/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0057\n",
      "Epoch 181/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0057\n",
      "Epoch 182/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0060\n",
      "Epoch 183/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0057\n",
      "Epoch 184/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0056\n",
      "Epoch 185/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0057\n",
      "Epoch 186/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0056\n",
      "Epoch 187/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0056\n",
      "Epoch 188/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0059\n",
      "Epoch 189/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0064\n",
      "Epoch 190/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0058\n",
      "Epoch 191/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0056\n",
      "Epoch 192/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0059\n",
      "Epoch 193/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0057\n",
      "Epoch 194/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0059\n",
      "Epoch 195/200\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0057\n",
      "Epoch 196/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0057\n",
      "Epoch 197/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0057\n",
      "Epoch 198/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0056\n",
      "Epoch 199/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0058\n",
      "Epoch 200/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0056\n"
     ]
    }
   ],
   "source": [
    "# Entrenar el modelo\n",
    "history = model.fit(X, y, epochs=200, verbose=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicción: -0.56533706\n"
     ]
    }
   ],
   "source": [
    "# Hacer predicciones\n",
    "x_input = tf.convert_to_tensor([[i] for i in range(100, 104)])\n",
    "x_input = tf.reshape(x_input, (1, n_steps, n_features))\n",
    "y_pred = model.predict(x_input, verbose=0)\n",
    "\n",
    "print(\"Predicción:\", y_pred[0][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
